# Week-10-Lecture
Week-10-Lecture

### 강의 주제: 한국의 'AI 윤리기준'부터 유네스코의 '권고안'까지, 세계 각국은 왜 인공지능을 '인간성'에 기초하여 규제하려 할까
  
### 1. 손실함수: 여행자가 ‘얼마나 길을 잘못 들었는지 알려주는 나침반’
* 개념: 델이 얼마나 틀렸는지 숫자로 알려주는 오류 측정기입니다.
* 예를 들어, 고양이를 보고 개인 줄 알았으면 → 오류가 큼 → 손실 높음
* 고양이를 보고 고양이라 했으면 → 오류 작음 → 손실 낮음
* 즉, 손실이 낮아지는 방향이 곧 더 똑똑해지는 방향입니다.
* 손실함수 종류: MSE(평균제곱오차), Cross Entropy(크로스 엔트로피), MAE(평균절대오차)등이 있습니다.

### 2. 경사하강법: “더 낮은 곳으로 내려가라”라고 안내하는 여행 방식
* 경사하강법(Gradient Descent)은 여행자가 ‘경사(기울기)’를 보고 방향을 정하는 방법입니다.
* 비유하면, 손실함수 = 산의 높이를 나타내는 지형
* 경사 = 지금 서 있는 자리의 기울기
* 경사하강법 = 기울기 방향을 따라 내려가는 행동
* 즉, 손실을 줄이는 방향을 따라 조금씩 내려오면서 최적점에 도달하는 과정=딥러닝의 학습
  
### 3. Adagrad: “많이 방문한 길은 조심해서, 처음 가는 길은 과감하게”
* 자주 다녀본 길(자주 업데이트된 파라미터)은 천천히 걷고, 거의 가보지 않은 길은 크게 보폭을 넓혀 걷는다.
* 많이 변한 파라미터는 학습률을 자동으로 작게 만들어 과적합을 방지
* 드물게 업데이트된 파라미터는 학습률을 크게 하여 稀疏(sparse) 특징 처리에 유리
* 자연어처리에서 단어 빈도 차이가 크기 때문에 Adagrad가 유용한 경우가 많음.
* 단점: 학습률이 지나치게 작아져서 결국 거의 걷지 못하게 되는 문제 발

### 3. RMSProp: “너무 오래 과거만 기억하지 말고, 최근 지형을 더 반영하자”

* Adagrad의 단점을 해결하기 위해, 최근 변화(최근 경사)를 더 많이 기억하고, 오래된 변화는 잊어버리는 방식이다.
* 지형의 ‘최근 경사’를 기준으로 학습률을 조절하는 여행 전략
* CNN, RNN, LSTM 학습에서도 널리 이용

### 4. Adam: Adagrad·RMSProp의 한계를 넘어선 “더 똑똑한 여행자”
* 딥러닝을 “손실을 줄이기 위해 산을 내려가는 여행”이라고 보면, 각 알고리즘의 차이는 어떻게 걸음을 조절하느냐의 차이.
* Adagrad → 너무 조심스러워져서 멈춤
* RMSProp → 조심은 하지만 방향감각이 부족
* Adam → 방향성과 속도 조절을 모두 챙긴 지능형 등산가

### 5. 딥러닝&손실함수&경사하강법 정리
* 손실함수가 “어디가 낮은 곳인지” 보여주고
* 경사하강법이 “내려가는 방향”을 알려주며
* 최적화기법(Adagrad, RMSprop, Adam)은 “걸음 크기와 속도를 조절하여 더 안전하고 빠르게 내려가게” 해준다.
* 딥러닝 전체는 손실이라는 높은 산에서 최저점을 찾아 내려가는 긴 여행이며,
* 이 여정을 얼마나 효율적으로 하느냐가 성능을 결정한다.

### 6. 감성컴퓨팅과 손실함수, 경사하강법, ADAM은 어떤 연관이 있을까?
* 딥러닝 기반 감성컴퓨팅에서는 문장·음성·표정의 감정을 정확히 예측해야 하고,
* 이때 예측이 틀리면 그 차이를 손실함수(loss)로 계산해 “얼마나 틀렸는지” 숫자로 만든다.
* 경사하강법은 이 손실을 줄이기 위해 가장 내려가는 방향으로 파라미터를 조금씩 조정하는 알고리즘이다.
* 하지만 감성 데이터는 노이즈·복잡한 패턴이 많아 단순 경사하강법은 불안정해,
* ADAM 같은 최적화 기법이 학습 속도를 빠르게 하고, 감정 분류 정확도를 높이며 안정적으로 수렴하도록 도와준다.


### 참고 파일

* 경사하강법.pdf
* 손실함수+엔트로피+크로스엔트로피+KL발살.pdf

